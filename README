
Introduction:
-------------

This package provides a small facility to analyze one or a chain of root ntuples.

A script (./scripts/make_rootNtupleClass.sh) is used to generate automatically
(using the root command RootNtupleMaker->MakeClass) a class (include/rootNtupleClass.h
and src/rootNtupleClass.C) with the variable definitions of a given root ntuple
(to be provided by the user).

The class baseClass (include/baseClass.h and src/baseClass.C) inherits from the
automatically generated rootNtupleClass.
baseClass provides the methods that are common to all analysis, such as the method
to read a list of root files and form a chain. It will, asap, also provide a method
to read a list of selection cuts.

The class analysisClass (include/analysisClass.h and src/analysisClass.C) inherits
from baseClass.
The user's code should be placed in the method Loop() of analysisClass, which reimplements
the method Loop() of rootNtupleClass.

The main program (src/main.C) receives the configuration parameters (such as the input
chain of root files and a file to provide a cut list) and executes the analysisClass code.

Instructions:
-------------

1) Environment setup:
/cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.16.00/x86_64-centos7-gcc48-opt/bin/thisroot.sh

2) Checkout the code:
   git clone git@github.com:CMSLQ/rootNtupleAnalyzerV2 Leptoquarks/analyzer/rootNtupleAnalyzerV2
You will probably also want the macros package:
   git clone git@github.com:CMSLQ/rootNtupleMacrosV2 Leptoquarks/analyzer/rootNtupleMacrosV2
Here we insert an extra directory so that scram will ignore the source files here 
(in case you have other CMSSW stuff in that area).

3) Generate the rootNtupleClass:
   cd rootNtupleAnalyzerV2/
   ./scripts/make_rootNtupleClass.sh
      (you will be asked for input arguments, such as an ntuple file to do MakeClass() on)

4) Copy the analysis template file into your own file:
     cp -i src/analysisClass_template.C src/analysisClass_myCode.C
   and make a symbolic link analysisClass.C by:
     ln -s analysisClass_myCode.C src/analysisClass.C

   NOTE: the macro analysisClass_template.C might not be compatible with the dataformat of your ntuples
         you have to write a consistent macro.
   NOTE2: There are many analysis classes already available in the macros repo.

5) Compile to test that all is OK so far (in order to compile, steps 2 and 3 need to be done first):
   make clean
   make
Note that if you run on full RootTupleMakerV2-produced ntuples, you have to use Makefile_fullNtuple:
   make -f Makefile_fullNtuple

6) If you are writing a new analysis class, add your analysis code to the method Loop() of analysisClass_myCode.C

   NOTE: to access variables use
   variable->at(i)
   instead of
   variable[i]

7) Compile as in 4.

8) Run:
   ./main
      (you will be asked for input arguments)

Note1:
  one can have several analyses in a directory, such as
    src/analysisClass_myCode1.C
    src/analysisClass_myCode2.C
    src/analysisClass_myCode3.C
  and move the symbolic link to the one to be used:
    ln -sf analysisClass_myCode2.C src/analysisClass.C
  and compile/run as above.

Note2:
  a github area to commit all the analysis macros such as analysisClass_XXX.C
  has been prepared (as noted above) here:
    https://github.com/CMSLQ/rootNtupleMacrosV2
  This will allow separate development of the rootNtupleAnalyzerV2 package
  from the development of the analysis macros.
  In order to compile and run an analysis macro
    /...fullPath.../rootNtupleMacros/src/analysisClass_XXX.C
  do:
    ln -sf /...fullPath.../rootNtupleMacrosV2/src/analysisClass_XXX.C src/analysisClass.C
  and compile/run as above.

More details:
-------------

- Example code:
The src/analysisClass_template.C comes with simple example code. The example code in enclosed by
  #ifdef USE_EXAMPLE
    ... code ...
  #endif //end of USE_EXAMPLE
The code is NOT compiled by default. In order to compile it, uncomment the line
  #FLAGS += -DUSE_EXAMPLE
in the Makefile.

- Providing cuts via file:
A list of cut variable names and cut limits can be provided through a file (see config/cutFileExample.txt).
The variable names in such a file have to be filled with a value calculated by the user analysisClass code,
a function "fillVariableWithValue" is provided - see example code.
Once all the cut variables have been filled, the cuts can be evaluated by calling "evaluateCuts" - see
example code. Do not forget to reset the cuts by calling "resetCuts" at each event before filling the
variables - see example code.
The function "evaluateCuts" determines whether the cuts are satisfied or not, stores the pass/failed result
of each cut, calculates cut efficiencies and fills histograms for each cut variable (binning provided by the
cut file, see config/cutFileExample.txt).
The user has access to the cut results via a set of functions (see include/baseClass.h)
  bool baseClass::passedCut(const string& s);
  bool baseClass::passedAllPreviousCuts(const string& s);
  bool baseClass::passedAllOtherCuts(const string& s);
where the string to be passed is the cut variable name.
The cuts are evaluated following the order of their apperance in the cut file (config/cutFileExample.txt).
One can simply change the sequnce of line in the cut file to have the cuts applied in a different order
and do cut efficiency studies.
Also, the user can assign to each cut a level (0,1,2,3,4 ... n) and use a function
  bool baseClass::passedAllOtherSameLevelCuts(const string& s);
to have the pass/failed info on all other cuts with the same level.
There is actually also cuts with level=-1. These cuts are not actually evaluated, the corresponding lines
in the cut file (config/cutFileExample.txt) are used to pass values to the user code (such as fiducial
region limits). The user can access these values (and also those of the cuts with level >= 0) by
  double baseClass::getCutMinValue1(const string& s);
  double baseClass::getCutMaxValue1(const string& s);
  double baseClass::getCutMinValue2(const string& s);
  double baseClass::getCutMaxValue2(const string& s);

- Automatic histograms for cuts
The following histograms are generated for each cut variable with level >= 0:
  no cuts applied
  passedAllPreviousCuts
  passedAllOtherSameLevelCuts
  passedAllOtherCuts
  passedAllCut
and by default only the following subset
  no cuts applied
  passedAllPreviousCuts
  passedAllOtherCuts
is saved to the output root file. All histograms can be saved to the output root file by
uncommenting the following line in the Makefile
#FLAGS += -DSAVE_ALL_HISTOGRAMS


- Automatic cut efficiency:
the absolute and relative efficiency is calculated for each cut and stored in an output file
(named data/output/cutEfficiencyFile.dat if the code is executed following the examples)

The user has the option to implement a good run list using a JSON file.  This requires two edits to the cut 
file and one edit to the analysisClass.C file.
  A line must be inserted at the beginning of the cut file with the word "JSON" first, and then 
    the full AFS path of the desiredJSON file. For example:
    JSON /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions11/7TeV/Prompt/Cert_160404-163369_7TeV_PromptReco_Collisions11_JSON.txt
  In addition, the user must define the JSON file selection in the cut file.  This is done in the usual way:
    #VariableName                   minValue1(<) maxValue1(>=)      minValue2(<)    maxValue2(>=)   level   histoNbinsMinMax
    #------------                   ------------ -------------      ------------    -------------   -----   ----------------
    PassJSON                        0            1                  -               -               0       2 -0.5 1.5
  In the analysisClass.C file, the user must add the following line within the analysis loop:
    fillVariableWithValue ( "PassJSON", passJSON (run, ls, isData));

Note that the use of a JSON file (good run list) is optional.  If the user does not list a JSON file in the cut file,
no selection will be made.

This tool was validated on May 18, 2011.  The results of the validation study are here:
https://twiki.cern.ch/twiki/pub/CMS/ExoticaLeptoquarkAnalyzerRootNtupleMakerV2/json_validation.pdf

Additional scripts for running on several datasets:
---------------------------------------------------

See ./doc/howToMakeAnalysisWithRootTuples.txt



Using the Optimizer (Jeff Temple):
----------------------------------

The input cut file can also specify variables to be used in optimization studies.
To do so, add a line in the file for each variable to optimize. The first field of a line
must be the name of the variable, second field must be "OPT", third field either ">" or "<".
(The ">" sign will pass values greater than the applied threshold, and "<" will pass
those less than the threshold.) 4th and 5th fields should be the minimum
and maximum thresholds you wish to apply when scanning for optimal cuts.
An example of the optimization syntax is:

#VariableName     must be OPT   > or <    RangeMin        RangeMax        unused
#------------     -----------   ------    ------------    -------------   ------
muonPt               OPT          >          10              55              5

This optimizer will scan 10 different values, evenly distributed over
the inclusive range [RangeMin, RangeMax]. At the moment, the 6th value is not used and
does not need to be specified.
The optimization cuts are always run after all the other cuts in the file, and are only run
when all other cuts are passed.
The above line will make 10 different cuts on muonPt, at [10, 15, 20, 25, ..., 55].
('5' in the 6th field is meaningless here.)
The output of the optimization will be a 10-bin histogram, showing the number of
events passing each of the 10 thresholds.

Multiple optimization cuts may be applied in the same file.  In the case where N optimization cuts
are applied, a histogram of 10^N bins will be produced, with each bin corresponding to a unique cut combination.
No more than 6 variables may be optimized at one time (limitation in the number of bins for a TH1F ~ 10^6).
Since such file can become quite large, the default is to not create

A file (optimizationCuts.txt in the working directory) that lists the cut values applied for
each bin can be produced by uncommenting the line
#FLAGS += -DCREATE_OPT_CUT_FILE
in the Makefile. Since this file can be quite large (10^N lines), by default it is not created.



Producing an ntuple skim (Dinko Ferencek):
------------------------------------------

The class baseClass provides the ability to produce a skimmed version of the input ntuples. In order to
produce a skim, the following preliminary cut line has to be added to the cut file

#VariableName         value1            value2          value3          value4          level
#------------         ------------      -------------   ------------    -------------   -----
produceSkim           1                 -               -               -               -1

and call the fillSkimTree() method for those events that meet the skimming criteria. One possible example is

    if( passedCut("all") ) fillSkimTree();

If the above preliminary cut line is not present in the cut file, is commented out or its value1 is set to 0,
the skim creation will be turned off and calling the fillSkimTree() method will have no effect.



JSON parser (Edmund Berry):
---------------------------

See https://hypernews.cern.ch/HyperNews/CMS/get/exotica-lq/266.html



PU reweight (Edmund Berry):
---------------------------

See https://twiki.cern.ch/twiki/pub/CMS/Exo2011LQ1AndLQ2Analyses/PileupReweightingCode.pdf
Some of the above is outdated.
1) To get the PILEUP_DATA_ROOT_FILE
Recipe is documented here: https://twiki.cern.ch/twiki/bin/view/CMS/PileupJSONFileforData#Calculating_Your_Pileup_Distribu
And also for 2015 especially: https://twiki.cern.ch/twiki/bin/view/CMS/PileupJSONFileforData#2015_Pileup_JSON_Files
See also: https://hypernews.cern.ch/HyperNews/CMS/get/physics-validation/2451/1/2/1/1/2/2.html
Lumi/pileup JSON can be found here: /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions15/13TeV/PileUp/pileup_latest.txt
See https://hypernews.cern.ch/HyperNews/CMS/get/luminosity/522.html
Example command:
pileupCalc.py -i ../../submitJobsWithCrabV2/runData2015D_singleElectron/v1-4-0_2015Oct25_093223/crab_SingleElectron__Run2015D-05Oct2015-v1/results/lumiSummary.json --inputLumiJSON /afs/cern.ch/cms/CAF/CMSCOMM/COMM_DQM/certification/Collisions15/13TeV/PileUp/pileup_latest.txt --calcMode true  --minBiasXsec 80000 --maxPileupBin 80 --numPileupBins 80  Pileup_SingleElectron__Run2015D-05Oct2015-v1.root
2) To get the PILEUP_MC_TXT_FILE
For 2015 MC at 25 ns, the pileup distribution used is here: SimGeneral/MixingModule/python/mix_2015_25ns_Startup_PoissonOOTPU_cfi.py
See here: https://twiki.cern.ch/twiki/bin/viewauth/CMS/PdmVPileUpDescription#Startup2015
This was checked with this dataset: /DYJetsToLL_M-50_HT-100to200_TuneCUETP8M1_13TeV-madgraphMLM-pythia8/RunIIWinter15GS-MCRUN2_71_V1-v1/GEN-SIM
Simply take the contents of mix.input.nbPileupEvents.probValue and put it in a file.
For 2015 this is in github.



Producing a new ntuple with a subset of cutFile variables and a subset of events (Paolo, Francesco, Edmund):
------------------------------------------------------------------------------------------------------------

The class baseClass provides the ability to produce a new ntuple with a subset of the variables defined
in the cutFile, and with a subset of events.
In order to do so, the following preliminary cut line has to be added to the cut file

#VariableName         value1            value2          value3          value4          level
#------------         ------------      -------------   ------------    -------------   -----
produceReducedSkim              1               -               -               -               -1

then each variable that needs to be included in the new tree has to be flagged with SAVE in 
the cutFile at the end of the line where the variabole is defined, as for pT1stEle and pT2ndEle
below:

#VariableName	      minValue1(<) maxValue1(>=)	minValue2(<)	maxValue2(>=)	level	histoNbinsMinMax  OptionalFlag
#------------	      ------------ -------------	------------	-------------	-----	----------------  ------------
nEleFinal	      1		   +inf			-		-		0	11 -0.5 10.5
pT1stEle              85           +inf                 -               -               1       100 0 1000        SAVE
pT2ndEle	      30	   +inf			-	        -	        1	100 0 1000        SAVE
invMass_ee	      0		   80			100	        +inf	        1	120 0 1200

(do not put anything for those variables that do not need to be saved, such as for  nEleFinaland invMass_ee)

finally, call fillReducedSkimTree() in the analysisClass for the subset of events that need to be saved, e.g.:

    if( passedCut("nEleFinal") ) fillReducedSkimTree();

If the above preliminary cut line is not present in the cut file, is commented out or its value1 is set to 0,
the skim creation will be turned off and calling the fillReducedSkimTree() method will have no effect.
The new ntuple will be created in a file named as the std output root file with _reduced_skim appended
before the .root and the tree name will be as in the input root file.


Quick guide to skimming code:
------------------------------------------------------------------------------------------------------------
1) Log into lxplus and do "cmsenv" in a CMSSW environment

2)  Check out the rootNtupleAnalyzerV2 and rootNtupleMacrosV2 packages:
cvs checkout -d rootNtupleAnalyzerV2 UserCode/Leptoquarks/rootNtupleAnalyzerV2
cvs checkout -d rootNtupleMacrosV2 UserCode/Leptoquarks/rootNtupleMacrosV2

3) Set environment variables:

export LQANA=$PWD/rootNtupleAnalyzerV2
export LQMACRO=$PWD/rootNtupleMacrosV2
export LQDATA="some path where you can store a lot of output.  Probably a subdirectory of your afs "work" area.

4) Navigate to the working directory:

cd $LQANA

5) Remember that to run the LQ analysis code, you need three pieces:
- An analysisClass file, which is ready for you: $LQMACRO/src2012/analysisClass_lq1_skim.C src/analysisClass.C
- A cut file, which is ready for you: $LQMACRO/config2012/ReducedSkims/cutTable_lq1_skim_SingleEle_loose.txt
- An input list, which you must make for yourself using a script (see step 6)

6) Make the inputlist for the files you want to run on:

python scripts/createList.py -i /eos/cms/store/group/phys_exotica/leptonsPlusJets/RootNtuple/scooper/RootNtuple-V00-03-11-Summer12MC_DY2JetsToLL_ScaleSysts_MG_20131008_124216/ -o config/FullNtupleDatasets_Summer12MC_DY2JetsToLL_ScaleSysts_MG
-m root

... The input list you want will be here:
$LQANA/config/FullNtupleDatasets_Summer12MC_DY2JetsToLL_ScaleSysts_MGi/inputListAllCurrent.txt

7) Compile the analysis class you need:

unlink src/analysisClass.C
ln -s $LQMACRO/src2012/analysisClass_lq1_skim.C src/analysisClass.C
./scripts/make_rootNtupleClass.sh -f "root://eoscms//eos/cms/store/group/phys_exotica/leptonsPlusJets/RootNtuple/scooper/RootNtuple-V00-03-11-Summer12MC_DY2JetsToLL_ScaleSysts_MG_20131008_124216/DY2JetsToLL_M-50_scaleup_8TeV-madgraph__Summer12-START53_V7C-v1__AODSIM_9_1_lll.root" -t rootTupleTree/tree
make -f Makefile_fullNtuple clean
make -f Makefile_fullNtuple

8) Test the code:

./main config/FullNtupleDatasets_Summer12MC_DY2JetsToLL_ScaleSysts_MGi/DY2JetsToLL_M-50_scaledown_8TeV-madgraph__Summer12-START53_V7C-v1__AODSIM.txt $LQMACRO/config2012/ReducedSkims/cutTable_lq1_skim_SingleEle_loose.txt rootTupleTree/tree test test

9) You can run skim jobs on crab3 (recommended) by using: scripts/launchAnalysis_crab3.py
10) You can check the status of the jobs by using: scripts/multicrab.py

9') For running locally, like for analysis on preselection skims, prepare a list of commands by editing some lines within scripts like the following:
scripts/writeCommandsToRunOnMoreCutFiles_ForSkimToEOS_MakeSingleEleReducedSkim.sh

You will need to edit:
SUBDIR: Output summary files will be stored in "$LQDATA/$SUBDIR".  Name it whatever you want.
FULLEOSDIR: The actual miniskims will be stored in this EOS path.  Name it whatever you want.
INPUTLIST: This should be a path to the inputlist you make in Step 6.

Everything else you should be able to leave unchanged.

When you're ready, execute the script:
./writeCommandsToRunOnMoreCutFiles_ForSkimToEOS_MakeSingleEleReducedSkim.sh

10') Examine the commands you have created.

The script will produce a text file called something like: 
commandsToRunOnMoreCutFiles_MakeSingleEleReducedSkim_lxplus.txt

Look at the contents of the text file.  There should be two commands in it.  The first command launches the jobs.  The second command should only be executed when all jobs are finished -- it checks to make sure all jobs are complete.

11') Launch the jobs by executing the first command in the text file.

12') When the jobs are done, execute the second command in the text file and follow the instructions.  If the second command prints out "All jobs successful" : you're done!
